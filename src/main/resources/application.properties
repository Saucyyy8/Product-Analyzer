# App name
spring.application.name=productAnalyzer
spring.ai.ollama.base-url=http://localhost:11434

# Specify the Ollama model to be used by the ChatClient.
# This must match a model you have downloaded in Ollama (e.g., via `ollama pull llama3`).
spring.ai.ollama.chat.model=llama3:latest

# Optional: You can configure model-specific parameters like temperature.
# Temperature controls the creativity of the model's responses (e.g., 0.2 is more deterministic, 0.9 is more creative).
spring.ai.ollama.chat.options.temperature=0.7

# Logging configuration
logging.level.root=INFO
logging.level.com.smartReview.productAnalyzer=DEBUG
logging.level.org.springframework.web=INFO
logging.level.org.openqa.selenium=INFO
logging.pattern.console=%d{yyyy-MM-dd HH:mm:ss} - %msg%n


# Configuring cache

spring.cache.cache-names=Product
spring.cache.caffeine.spec=expireAfterAccess=60m

